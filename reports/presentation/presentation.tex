\documentclass[aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{default}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}

\usecolortheme{beaver}

\title[GPU Partitioning on SDumont II]{GPU Resource Partitioning on SDumont II}
\subtitle{NUMA Locality Impact on CPU-GPU Bandwidth}
\author{Pablo Alessandro Santos Hugen}
\institute{Universidade Federal do Rio Grande do Sul - Instituto de Inform√°tica}
\date{December 2025}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Outline}
    \tableofcontents
\end{frame}

\section{Introduction}

\begin{frame}{Motivation}
    \textbf{SDumont II} GH200 node allocation modes:
    \begin{itemize}
        \item \textbf{Exclusive}: Full node for one job
        \item \textbf{Shared}: SLURM GRES partitions resources
    \end{itemize}

    \vspace{1em}
    \textbf{Questions:}
    \begin{enumerate}
        \item How does bandwidth vary with NUMA affinity?
        \item Does SLURM preserve locality in shared mode?
    \end{enumerate}
\end{frame}

\begin{frame}{GH200 Node Architecture}
    \begin{columns}
        \column{0.5\textwidth}
        \textbf{Specifications:}
        \begin{itemize}
            \item 4x NVIDIA GH200 GPUs (120GB HBM3)
            \item 288 ARM cores (72 per NUMA)
            \item NVLink-C2C: 900 GB/s (local)
            \item NVLink 4.0 between GPUs
        \end{itemize}

        \vspace{1em}
        \textbf{NUMA affinity:}
        \begin{itemize}
            \item GPU 0 $\leftrightarrow$ NUMA 0
            \item GPU 1 $\leftrightarrow$ NUMA 1
            \item GPU 2 $\leftrightarrow$ NUMA 2
            \item GPU 3 $\leftrightarrow$ NUMA 3
        \end{itemize}

        \column{0.5\textwidth}
        \begin{center}
            \includegraphics[width=\textwidth]{../paper/img/exclusive.png}
        \end{center}
    \end{columns}
\end{frame}

\section{Methodology}

\begin{frame}{Experimental Setup}
    \textbf{Tool:} nvbandwidth v0.6

    \vspace{1em}
    \textbf{Exclusive queue:}
    \begin{itemize}
        \item Pin process to each NUMA (0-3) with \texttt{numactl}
        \item Measure bandwidth to all 4 GPUs
        \item Result: 4$\times$4 bandwidth matrix
    \end{itemize}

    \vspace{1em}
    \textbf{Shared queue:}
    \begin{itemize}
        \item Submit 4 concurrent jobs with 1 GPU each
        \item Observe SLURM's NUMA-GPU mapping
    \end{itemize}
\end{frame}

\section{Results}

\begin{frame}{Host-to-Device Bandwidth}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{../paper/img/bandwidth_heatmaps.png}
    \end{center}

    \begin{itemize}
        \item \textbf{Local (diagonal):} 411.6 GB/s
        \item \textbf{Remote (off-diagonal):} 88.6 GB/s
        \item \textbf{Ratio:} \textcolor{red}{\textbf{4.65$\times$}}
    \end{itemize}
\end{frame}

\begin{frame}{CPU-GPU Latency}
    \begin{columns}
        \column{0.5\textwidth}
        \begin{center}
            \includegraphics[width=\textwidth]{../paper/img/latency_heatmap.png}
        \end{center}

        \column{0.5\textwidth}
        \begin{itemize}
            \item Local: \textbf{739 ns}
            \item Remote: \textbf{1109 ns}
            \item Ratio: 1.5$\times$
        \end{itemize}
    \end{columns}
\end{frame}

\begin{frame}{Exclusive vs Shared Queue}
    \begin{center}
        \includegraphics[width=0.65\textwidth]{../paper/img/bandwidth_comparison.png}
    \end{center}

    \begin{itemize}
        \item Shared queue achieves $\sim$100\% of local bandwidth
        \item SLURM preserves NUMA locality automatically
    \end{itemize}
\end{frame}

\section{Application Validation}

\begin{frame}{GROMACS Benchmark}
    \textbf{Setup:} GROMACS 2023.2, STMV ($\sim$1M atoms), GPU offload: nb, bonded, pme

    \vspace{0.5em}
    \begin{table}
        \centering
        \begin{tabular}{llcc}
            \toprule
            \textbf{Queue} & \textbf{GPUs} & \textbf{ns/day} & \textbf{Scaling} \\
            \midrule
            Exclusive & 1 & 43.45 & 1.00$\times$ \\
            Exclusive & 2 & 42.25 & 0.97$\times$ \\
            Exclusive & 4 & 108.75 & \textbf{2.50$\times$} \\
            \midrule
            Shared & 1 & 43.45 & 1.00$\times$ \\
            Shared & 2 & 42.28 & 0.97$\times$ \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}{GROMACS GPU Scaling}
    \begin{center}
        \includegraphics[width=0.75\textwidth]{../paper/img/gromacs_scaling.png}
    \end{center}

    \begin{itemize}
        \item \textbf{1-GPU:} Same performance in both queues
        \item \textbf{2-GPU:} No scaling benefit
        \item \textbf{4-GPU:} 2.5$\times$ speedup
    \end{itemize}
\end{frame}

\begin{frame}{Summary}
    \begin{table}
        \centering
        \begin{tabular}{lc}
            \toprule
            \textbf{Metric} & \textbf{Value} \\
            \midrule
            H2D Bandwidth (Local) & 411.6 GB/s \\
            H2D Bandwidth (Remote) & 88.6 GB/s \\
            D2H Bandwidth (Local) & 169.4 GB/s \\
            D2H Bandwidth (Remote) & 88.3 GB/s \\
            \midrule
            Local/Remote Ratio & \textbf{4.65$\times$} \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{frame}

\section{Conclusions}

\begin{frame}{Conclusions}
    \textbf{Bandwidth:}
    \begin{itemize}
        \item Local: 411 GB/s | Remote: 88 GB/s (4.6$\times$ difference)
        \item Shared queue maintained NUMA locality
    \end{itemize}

    \vspace{0.5em}
    \textbf{GROMACS (STMV):}
    \begin{itemize}
        \item Single-GPU: Same performance in both queues
        \item 4-GPU: 2.5$\times$ speedup | 2-GPU: No benefit
    \end{itemize}

    \vspace{0.5em}
    \textbf{Guidance:}
    \begin{itemize}
        \item Single-GPU jobs: Shared queue has no penalty
        \item Multi-GPU GROMACS (STMV): Use 1 or 4 GPUs
    \end{itemize}
\end{frame}

\begin{frame}{}
    \begin{center}
        \Large\textbf{Thank you!}

        \vspace{1em}
        \normalsize Questions?
    \end{center}
\end{frame}

\end{document}
